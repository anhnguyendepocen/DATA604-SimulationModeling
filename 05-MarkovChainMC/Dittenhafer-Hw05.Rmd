---
title: "Homework 5"
subtitle: "DATA604 Simulation and Modeling"
author: "Daniel Dittenhafer"
date: "April 10, 2016"
output: pdf_document
classoption: portrait
geometry: margin=0.5in
bibliography: references.bib
csl: ../emerald-harvard.csl
---
```{r, echo=FALSE, message=FALSE}
library(knitr)
#library(gplots)
library(ggplot2)
library(randtoolbox)
library(gridExtra)
#library(plot3D)
#library(tseries)
set.seed(20275)
# My ggplot theme
myTheme <- theme(axis.ticks=element_blank(),
                 axis.title=element_text(size="10"),
                  panel.border = element_rect(color="gray", fill=NA), 
                  panel.background=element_rect(fill="#FBFBFB"), 
                  panel.grid.major.y=element_line(color="white", size=0.5), 
                  panel.grid.major.x=element_line(color="white", size=0.5),
                  plot.title=element_text(size="10"))
```

```{r, echo=FALSE, message=FALSE}
library(knitcitations)
library(RefManageR)

cleanbib()

cite_options(style="markdown")

bibPkgTseries <- bibentry(bibtype="Misc",
                 author=personList(person(family="Trapletti", given="Adrian"), 
                                   person(family="Hornik", given="Kurt")),
                 journal="Annual Review of Sociology",
                 title="tseries: Time Series Analysis and Computational Finance",
                 year=2015,
                 note="R package version 0.10-34",
                 url="http://CRAN.R-project.org/package=tseries")
```

# 1

*At the start of each week, the condition of a machine is determined by measuring the amount of electrical current it uses. According to its amperage reading, the machine is categorized as being in one of the following four states: low, medium, high and failed. A machine in the low state has a probability of 0.05, 0.03, and 0.02 of being in the medium, high, or failed state, respectively, at the start of the next week. A machine in the medium state has a probability of 0.09 and 0.06 of being in the high or failed state, respectively, at the start of the next week (it cannot, by itself, go to the low state). And, a machine in the high state has a probability of 0.1 of being in the failed state at the start of the next week (it cannot, by itself, go to the low or medium state). If a machine is in the failed state at the start of a week, repair is immediately begun on the machine so that it will (with probability 1) be in the low state at the start of the following week. Let X be a Markov chain where Xn is the state of the machine at the start of week n.*

## a) Given the Markov transition matrix for \(X\).

The following code creates and prints the Markov transition matrix for \(X\).

```{r}
tmX <- matrix(c(0.90, 0.05, 0.03, 0.02, 
                0, 0.85, 0.09, 0.06,
                0, 0.00, 0.90, 0.10,
                1, 0.00, 0.00, 0.00), nrow=4, byrow=TRUE)
kable(tmX)
```

## b) A new machine always starts in the low state. What is the probability that the machine is in the failed state three weeks after it is new?

First we define a helper function:

```{r}
markovChainStep <- function(initialState, tranMatrix, steps)
{
  n <- steps
  step <- initialState
  for(i in 1:n)
  {
    step <- step %*% tranMatrix
  }
  return (step)
}
```

Next we call the helper function to compute the requested probability. We step twice in the function as it is 2 steps beyond the initial probability state.

```{r}
bStep <- markovChainStep(tmX, tmX, 2)
kable(bStep)
```

The probability that the machine is in the failed state 3 weeks after it is new is `r bStep[1,4]`.


## c) What is the probabililty that a machine has at least one failure three weeks after is is new?

Compute the steady state probabilities for this recurrent transition matrix.

```{r}
steadyState <- markovChainStep(tmX, tmX, 100)
steadyState

steadyState[1,4]
```


```{r}
Fm <- matrix(c(1, 1, 1, 1,
               1, 1, 1, 1,
               1, 1, 1, 1,
               1, 1, 1, 1), byrow=TRUE, nrow=4)
Fm

Rm <- matrix(nrow=4, ncol=4)
for(i in 1:nrow(Fm))
{
  for(j in 1:ncol(Fm))
  {
    if(i == j)
    {
      Rm[i,j] <- 1 / (1 - Fm[j,j])
    }
    else
    {
      Rm[i,j] <- Fm[i,j] / (1 - Fm[j,j])
    }
  }
}
  
Rm
```


## d) What is the expected number of weeks after a new machine is installed until the first failure occurs?

```{r}
1 / steadyState[1,4]
```

## e) On average, how many weeks per year is the machine working?

```{r}
# Use Metropolis-Hastings to estimate?
```

## f) Each week that the machine is in the low state...

**a profit of $1000 is realized; each eweek that the machine is in the medium state, a profit of $500 is realized; each week tha the machine is in the high state, a profit of $400 is realized... What is the long-run average profit per week realized by the machine?**

```{r}
PL <- c(1000, 500, 400, -700)
PL

lrAvgProfit <- PL %*% steadyState[1, ]
lrAvgProfit
```

## g) Policy change?

Define the new profit and loss values:

```{r}
newPL <- c(1000, 500, -600, -700)
newPL
```

Define the new transition matrix:

```{r}
newX <- matrix(c(0.90, 0.05, 0.03, 0.02, 
                    0, 0.85, 0.09, 0.06,
                    1, 0.00, 0.00, 0.00,
                    1, 0.00, 0.00, 0.00), nrow=4, byrow=TRUE)
kable(newX)
```

Compute the new steady state and long-run average profit:

```{r}
newSteadyState <- markovChainStep(newX, newX, 100)

newLrAvgProfit <- newPL %*% newSteadyState[1, ]
newLrAvgProfit
```

At an average profit of $ `r round(newLrAvgProfit, 2)`, an \(\approx `r round(newLrAvgProfit - lrAvgProfit, 2)`\) increase, the new policy is worthwhile.

# 2

**Rao (1973) presented an example on genetic linkage of 197 animals in four categories. The group sizes are (125, 18, 20, 34). Assume that the probabilities of the corresponding multinomial distribution are ... **


```{r}
groupSizes <- c(125, 18, 20, 34)

w <- 0.25
m <- 5000
burn <- 1000
#days <-  250
x <- numeric(m)

# Function to wrap the computation of the target density.
prob <- function(theta, K) {
  p <- (0.5 + (theta / 4))^K[1] * (1 - theta / 4)^K[2] * (1 - theta / 4)^K[3] * (theta/4)^K[4] 
}
# Random Walk - Metropolis algorithm
u <- runif(m)
v <- runif(m, -w, w)
x[1] <- 0.25
for(i in 2:m) {
  y <- x[i-1] + v[i]
  if(u[i] <= prob(y, groupSizes) / prob(x[i-1], groupSizes)) {
    x[i] <- y 
  } else {
    x[i] <- x[i-1]
  }
}
# Use mean to distill the MCMC results to a single value
xb <- x[burn + 1: m]
theta.hat <- mean(xb, na.rm = TRUE)
print(theta.hat)
```

The estimate of \(\theta\) is `r theta.hat`. The following charts show the random walk Metropolis chain and resulting empirical distribution.

```{r, fig.height=3, message=FALSE, echo=FALSE}
dfX <- data.frame(index=seq(1, m), x=x)
g1 <- ggplot(dfX) + geom_line(aes(x=index, y=x)) + myTheme

g2 <- ggplot(dfX) + geom_histogram(aes(x=x)) + myTheme

grid.arrange(g1, g2, ncol=2)

```

# 3

Define the provided Y vector:

```{r}
Y <- c(4,5,4,1,0,4,3,4,0,6,3,3,4,0,2,6,3,3,5,4,5,3,1,4,4,1,5,5,3,4,2,5,2,2,3,4,2,1,3,2,2,1,1,1,1,3,0,0,1,0,1, 1,0,0,3,1,0,3,2,2,0,1,1,1,0,1,0,1,0,0,0,2,1,0,0,0,1,1,0,2,3,3,1,1,2,1,1,1,1,2,4,2,0,0,0,1,4,0,0,0,1,0,0,0,0,0,1,0,0,1,0,1)
```

Define the `fullcondm` function per the implementation pseudo code.

```{r}
fullcondm <- function(m, lambda, phi, y, n, alpha0, beta0, gamma0, delta0, verbose=FALSE)
{
  
  lamexp <- ifelse(m > 1, sum(y[1:m]), 0)
  phiexp <- ifelse(m < n, sum(y[(m+1):n]), 0)
  
  
  
  t1 <- lambda^(alpha0-1+lamexp)
  t2 <- exp(-(beta0+m)*lambda)
  t3 <- phi^(gamma0-1+phiexp)
  t4 <- exp(-(delta0+n-m)*phi)
  
  if(verbose == TRUE)
  {
    print(paste(t1, t2, t3, t4, lamexp, phiexp, sep=" "))  
  }
  
  return (t1*t2*t3*t4)
}
```


Define the `R` code for the Gibbs sample based onthe pseudo-code for implementation:

```{r}
gibbsSampler3 <- function(alpha0, gamma0)
{
  n <- length(Y)
  z <- 1000
  #lambda <- phi <- B <- delta <- numeric(z)
  #L <- numeric(n)
  
  lambda <- 1
  phi <- 1
  m <-  sample(1:n, 1)
  B <- 1
  delta <- 1
  
  Mprob <- numeric(n)
  mChain <- matrix(nrow=z, ncol=5)
  
  # Gibbs Sampler
  for(i in 1:z) {
    
    
    sum1 <- sum(Y[1:m])
    sum2 <- sum(Y[(m+1):n])
    
    lambda <- rgamma(1, shape=sum1+alpha0, scale=B / (B * m + 1))
    phi <- rgamma(1, shape=sum2+gamma0, scale=delta / ( delta * (n-m) + 1))
    
    for(j in 1:n) {
      Mprob[j] <- fullcondm(j, lambda, phi, Y, n, alpha0, B, gamma0, delta, FALSE)
    }
    
    m <- sample(1:n, size=1, prob=Mprob)
  
    bDone <- FALSE
    while(!bDone) {
      B <- 1 / rgamma(1, shape=alpha0, scale=lambda + 1)  
      if(B <= 99)
      {
        bDone <- TRUE
      }
    }    
    
    bDone <- FALSE
    while(!bDone) {
      delta <- 1 / rgamma(1, shape=gamma0, scale=phi + 1)
      if(delta <= 99)
      {
        bDone <- TRUE
      }
    }    
    
    mChain[i,] <- c(lambda, phi, m, B, delta)
    
  }
  
  return (mChain)
}

gibbsData <- gibbsSampler3(1, 1)


```

## a) Run your code for 5000 iterations. 

```{r, fig.height=3, echo=FALSE, message=FALSE}
dfMChain <- as.data.frame(gibbsData)
colnames(dfMChain) <- c("lambda", "phi", "m", "beta", "delta")
g1 <- ggplot(dfMChain) + 
  geom_histogram(aes(x=lambda)) + 
  myTheme +
  labs(title="Histogram of Lambda")
g2 <- ggplot(dfMChain) + 
  geom_histogram(aes(x=phi)) + 
  myTheme +
  labs(title="Histogram of phi")
g3 <- ggplot(dfMChain) + 
  geom_histogram(aes(x=m)) + 
  myTheme +
  labs(title="Histogram of m")

grid.arrange(g1, g2, g3, ncol=3)
```

```{r, fig.height=3, echo=FALSE, message=FALSE}
g1 <- ggplot(dfMChain) + 
  geom_line(aes(x=lambda, y=phi)) +
  myTheme +
  labs(title="Plot of Lambda vs Phi")
g2 <- ggplot(dfMChain) + 
  geom_line(aes(x=lambda, y=m)) + 
  myTheme +
  labs(title="Plot of Lambda vs m")
g3 <- ggplot(dfMChain) + 
  geom_line(aes(x=beta, y=delta)) +
  myTheme +
  labs(title="Plot of beta vs delta")

grid.arrange(g1, g2, g3, ncol=3)
```

## b) 

### When do you think the change point occured? 

```{r}
mMean <- mean(dfMChain$m)
mMean

roundedM <- round(mMean)
```

Rounding to a whole year, it appears `r roundedM` years would be change point. The 95% confidence interval is computed based on the standard deviation of the vector of m's generated from the MCMC simulation as shown below:

```{r}
n <- length(Y)
sdM <- sd(dfMChain$m)
tVal <- pt(0.975, df=n-1)
seM <- sdM / sqrt(n)

ci95 <- c(mMean - (tVal * sdM), mMean + (tVal * sdM))
ci95
```

### What are the average rates of coal mining accidents before and after the change?

The following code computes the before and after change point means:

```{r}
beforeMean <- mean(Y[1:roundedM])
beforeMean

afterMean <- mean(Y[(roundedM+1):n])
afterMean
```

As shown in the line chart below, there does appear to be a difference in the data prior to 41 versus after 41 (the blue line = 41).

```{r, echo=FALSE, fig.height=3}
g1 <- ggplot(data.frame(i=seq(1,n), accidents=Y)) + 
  geom_line(aes(x=i, y=accidents)) + 
  geom_vline(xintercept=roundedM, colour="lightblue") + 
  myTheme
g1
```


## c) How is Gibbs sampling different from the Metropolis-Hastings approach?

Gibbs sampling is a special case of Metropolis-Hastings, where every sampled point is accepted.

# 4) Traveling Salesman Problem

First define the cost matrix:

```{r}
C <- as.matrix(read.table("TravelingSalesmenCosts.csv", sep=","))
kable(C)
```

```{r}
costTsp <- function(sol, C)
{
  cost <- 0
  for(i in 2:length(sol))
  {
    a <- sol[i - 1]
    b <- sol[i]
    cost <- cost + as.numeric(C[a,b])
  }
  
  return (cost)
}
```

```{r}
simulatedAnnealingTsp <- function(T, z, beta, C)
{
  n <- nrow(C)
  results <- numeric(z)
  path <- matrix(nrow=z, ncol=n)
  
  x <- sample(1:17, 17, replace=FALSE)
  Sx <- costTsp(x, C)
  xbest <- x
  sbest <- Sx
  
  
  for( i in 1:z ) 
  {
    x <- sample(1:17, 17, replace=FALSE)
    ft <-  x[1:2]
    I <- ft[order(ft)]
    y <- x #c(x[1:I[1]-1], x[I[2]:-1:]
    
    Sy <- costTsp(y, C)
    alpha <- ifelse( Sy < Sx, 1, exp(-(Sy-Sx)/T))
    u <- runif(1, 0, 1)
    if(u < alpha)
    {
      x <- y
      Sx <- Sy
    }
    
    T <- beta * T
    
    xbest <- x
    sbest <- Sx
    
    results[i] <- sbest
    path[i,] <- x
  }
  
  return(list(costs=results, paths=path))
}

z <- 10000
T <- 1
beta <- 0.9999
res <- simulatedAnnealingTsp(T, z, beta, C)

# Minimum solution and path:
min(res$costs)
ndxMin <- which.min(res$costs)
res$paths[ndxMin,]
```

A random tour, for comparison:

```{r}
n <- nrow(C)
r <- sample(1:n, n, replace=FALSE)
costTsp(r, C)
```

The following histogram shows the distribution of the costs through the simulated annealing process:

```{r, echo=FALSE, fig.height=3, message=FALSE}
dfResult <- data.frame(Sx=res$costs)
g1 <- ggplot(dfResult) + 
  geom_histogram(aes(x=Sx)) +
  myTheme
g1
```

```{r}
z2 <- 5
costList <- numeric(z2)
for(i in 1:z2)
{
  res <- simulatedAnnealingTsp(T, z, beta, C)
  costList[i] <- min(res$costs)
}
```

```{r, echo=FALSE, fig.height=3}
g1 <- ggplot(data.frame(i=seq(1, z2), cost=costList)) + 
  geom_line(aes(x=i, y=cost)) + 
  myTheme
g1
```

# Experiment with Citations

The citation: `r citep("10.1111/j.0006-341X.1999.00001.x")`

Another citation approach: [@Mau_1999]

```{r, echo=FALSE, message=FALSE}
#
#BibOptions(style="html", bib.style="authortitle")
#bibliography()
write.bibtex(file="references.bib")
```

# References

