---
title: "Homework 4"
subtitle: "DATA604 Simulation and Modeling"
author: "Daniel Dittenhafer"
date: "March 22, 2016"
output: pdf_document
classoption: portrait
geometry: margin=0.5in
---
```{r, echo=FALSE, message=FALSE}
library(knitr)
#library(gplots)
library(ggplot2)
library(randtoolbox)
#library(gridExtra)
#library(plot3D)
#library(tseries)
set.seed(20275)
# My ggplot theme
myTheme <- theme(axis.ticks=element_blank(),
                 axis.title=element_text(size="10"),
                  panel.border = element_rect(color="gray", fill=NA), 
                  panel.background=element_rect(fill="#FBFBFB"), 
                  panel.grid.major.y=element_line(color="white", size=0.5), 
                  panel.grid.major.x=element_line(color="white", size=0.5),
                  plot.title=element_text(size="10"))
```

```{r, echo=FALSE, message=FALSE}
library(knitcitations)
library(RefManageR)

cleanbib()

cite_options(style="markdown")

bibPkgTseries <- bibentry(bibtype="Misc",
                 author=personList(person(family="Trapletti", given="Adrian"), 
                                   person(family="Hornik", given="Kurt")),
                 journal="Annual Review of Sociology",
                 title="tseries: Time Series Analysis and Computational Finance",
                 year=2015,
                 note="R package version 0.10-34",
                 url="http://CRAN.R-project.org/package=tseries")
```

# 1

*In this problem, you will implement and investigate a series of variance reduction procedures for Monte Carlo method by estimating the expected value of a cost function c(x) which depends on a D-dimensional random variable x.*

*The cost function is:*

\[c(x)=\frac{1}{(2\pi)^{\frac{D}{2}}} e^{-1/2 x^T x}\]

where

\[x_i \sim U(-5,5) \text{ for } i=1..D\]

Goal: estimate E[c(x)] - the expected value of c(x) - using Monte Carlo methods and see how it compares to the real value, which you are able to find by hand. 

```{r cost-function}
# First define the cost function as an R function
costFx <- function(x, D=-1)
{
  b <- exp(-0.5 * t(x) %*% x)
  if(D == -1)
  {
    D <- length(x)  
  }
  
  res <- (1 / ((2 * pi)^(D/2))) * b
  return (res)
}
```

## a) Crude Monte Carlo

```{r crude-monte-carlo}
crudeMC <- function(n, min, max, d = 1)
{
  theta.hat <- rep(NA, n)
  for(i in 1:n)
  {
    x <- runif(d, min, max)
    theta.hat[i] <- costFx(x)
  }
  
  return (theta.hat)
}

#ret <- crudeMC(10, -5, 5, 2)
#ret
#mean(ret)

montecarlo.Loop <- function(d, fun, verbose=FALSE)
{
  crudeMc.result <- data.frame(mean=c(), stdev=c(), n=c())
  for(n in seq(1000, 20000, by=1000))
  {
    res <- fun(n=n, min=-5, max=5, d=d)
    if(verbose)
    {
      #print("Data")
      #print(res)
      #print("Mean")
      #print(mean(res))
      print(dim(res))
    }
    
    crudeMc.result <- rbind(crudeMc.result, data.frame(mean=mean(res), stdev=sd(res), n=n))
  }
  
  crudeMc.result$EcActual <- (1/10)^d
  crudeMc.result$CoefVari <- crudeMc.result$stdev / crudeMc.result$mean
  
  return (crudeMc.result)
}
```

In the code below, we call the crude Monte Carlo loop function, show the top entries and visualize the result for D=1. The blue line represents the mean value, pink is the standard deviation, and the green line is the analytical value for \(E[c(x)] = (1/10)^D\).

```{r, cache=TRUE}
crudeMc.D1 <- montecarlo.Loop(d=1, fun=crudeMC)
```

```{r, echo=FALSE, fig.height=3}
g1 <- ggplot(crudeMc.D1) + 
  geom_point(aes(x=n, y=mean), colour="lightblue") + 
  geom_line(aes(x=n, y=mean), colour="lightblue") + 
  geom_point(aes(x=n, y=stdev), colour="pink") + 
  geom_line(aes(x=n, y=stdev), colour="pink") + 
  geom_line(aes(x=n, y=EcActual), colour="lightgreen") + 
  labs(title="D=1 Monte Carlo Estimate by n", y="mean/stdev") +
  myTheme
g1
```

```{r, echo=FALSE}
kable(crudeMc.D1)
```


In the code below, we call the crude Monte Carlo loop function, show the top entries and visualize the result for D=2.

```{r, cache=TRUE}
crudeMc.D2 <- montecarlo.Loop(d=2, fun=crudeMC, verbose=FALSE)
```

```{r, echo=FALSE, fig.height=3}
g1 <- ggplot(crudeMc.D2) + 
  geom_point(aes(x=n, y=mean), colour="lightblue") + 
  geom_line(aes(x=n, y=mean), colour="lightblue") + 
  geom_point(aes(x=n, y=stdev), colour="pink") +   
  geom_line(aes(x=n, y=stdev), colour="pink") +
  geom_line(aes(x=n, y=EcActual), colour="lightgreen") + 
  labs(title="D=2 Monte Carlo Estimate by n", y="mean/stdev") +
  myTheme
g1
```

```{r, echo=FALSE}
kable(crudeMc.D2)
```

## b) Quasi-Random Numbers

First, we compare the typical uniform random numbers from R's `runif` function to Sobol quasi-random numbers from `randtoolbox::sobol` function. 100 pairs of numbers are drawn from both generators and visualized below.

### Uniform Random Numbers

The following code segment uses `runif` to generate \(m=100\) random numbers and plots them.

```{r}
m <- 100
unifRn <- as.data.frame(matrix(runif(m * 2), ncol=2))
colnames(unifRn) <- c("x", "y")
head(unifRn)
```

```{r, echo=FALSE, fig.height=3}
g2 <- ggplot(data=as.data.frame(unifRn)) + 
  geom_point(aes(x=x, y=y)) + 
  labs(title="Uniform Psuedo-Random Numbers n=100 x 2 (runif)") +
  myTheme
g2
```

### Sobol Random Numbers

The following code segment uses `sobol` to generate \(m=100\) random numbers and plots them.


```{r}
sobolRn <- as.data.frame(sobol(m, d=2))
colnames(sobolRn) <- c("x", "y")
head(sobolRn)
```

```{r, echo=FALSE, fig.height=3}
g2 <- ggplot(data=as.data.frame(sobolRn)) + 
  geom_point(aes(x=x, y=y)) + 
  labs(title="Sobol Quasi-Random Numbers n=100 x 2 (randtoolbox::sobol)") +
  myTheme 
g2
```

At \(m=100\), the differences are less obvious, but there is some discernable pattern to the Sobol numbers which is more apparent at great \(m\). As such, the prefix "quasi" seems appropriate. The definition of "quasi" is "seemingly, apparently but not really". These might appear to be random numbers at first glace, but there is quite a pattern, the lattice, as more and more are generated.

### Sobol Monte Carlo

First, `sobol` based helper functions are defined using the same structure as the prior `runif` based functions. A couple of changes were needed:

* `sobol` returns a matrix. This is converted to a vector for use in the cost function.
* `sobol` has an init parameter, but we don't want to re-initialize every call, so this is bubbled up to the loop function to allow it to drive the re-init. 

```{r sobol-monte-carlo}
# Define a function to help us convert a 0-1 RV to a x-y RV 
rndRange <- function(x, min, max)
{
  r <- max - min
  p <- x * r
  new <- min + p
  return(new)
}

# Sobol Monte Carlo Inner function
sobolMC <- function(n, min, max, d = 1, init=TRUE)
{
  # Need a loop in here
  theta.hat <- rep(NA, n)
  for(i in 1:n)
  {
    x <- as.vector(sobol(n=1, d=d, init=init))
    x <- rndRange(x, min, max)
    theta.hat[i] <- costFx(x)
    init <- FALSE # turn off the init for i > 1 iterations
  }
  
  return (theta.hat)
}


#sobolMC(n=10, -5, 5, d=2)
```

In the code below, we call the Sobol Monte Carlo loop function, show the top entries and visualize the result for D=1. Again, the blue line represents the mean value, pink is the standard deviation, and the green line is the analytical value for \(E[c(x)] = (1/10)^D\).

```{r, cache=TRUE}
sobolMc.D1 <- montecarlo.Loop(d=1, fun=sobolMC)
```

```{r, echo=FALSE, fig.height=3}
g1 <- ggplot(sobolMc.D1) + 
  geom_point(aes(x=n, y=mean), colour="lightblue") + 
  geom_line(aes(x=n, y=mean), colour="lightblue") + 
  geom_point(aes(x=n, y=stdev), colour="pink") + 
  geom_line(aes(x=n, y=stdev), colour="pink") + 
  geom_line(aes(x=n, y=EcActual), colour="lightgreen") + 
  labs(title="D=1 Sobol Monte Carlo Estimate by n", y="mean/stdev") +
  myTheme
g1
```

```{r, echo=FALSE}
kable(sobolMc.D1)
```


```{r, cache=TRUE}
sobolMc.D2 <- montecarlo.Loop(d=2, fun=sobolMC)
```

```{r, echo=FALSE, fig.height=3}
g1 <- ggplot(sobolMc.D2) + 
  geom_point(aes(x=n, y=mean), colour="lightblue") + 
  geom_line(aes(x=n, y=mean), colour="lightblue") + 
  geom_point(aes(x=n, y=stdev), colour="pink") + 
  geom_line(aes(x=n, y=stdev), colour="pink") + 
  geom_line(aes(x=n, y=EcActual), colour="lightgreen") + 
  labs(title="D=2 Sobol Monte Carlo Estimate by n", y="mean/stdev") +
  myTheme
g1
```

```{r, echo=FALSE}
kable(sobolMc.D2)
```

### Compare Pure Uniform and Sobol

From what I see, the average of the sobol-based approach is closer to the expected value vs the more pure uniform approach. On the otherhand, the standard deviations are generally similar, though the sobol stdevs tend to drive to \(\approx 0.13494\) as whereas the uniform approach ranges 0.134 to 0.136 (a broader range). In other words, the sobol approach seems to result is a reduced variance of the variance.

```{r}
comparedMc.D1 <- data.frame(crudeMc.D1$mean, 
                            sobolMc.D1$mean, 
                            meanDiff=crudeMc.D1$mean - sobolMc.D1$mean,
                            crudeMc.D1$stdev, 
                            sobolMc.D1$stdev, 
                            stdevDiff=crudeMc.D1$stdev - sobolMc.D1$stdev,
                            n=crudeMc.D1$n)
kable(comparedMc.D1)
```

```{r, echo=FALSE, fig.height=3}
combinedStdev.D1 <- data.frame(type="crude", stdev=crudeMc.D1$stdev, n=crudeMc.D1$n)
combinedStdev.D1 <- rbind(combinedStdev.D1, data.frame(type="sobol", stdev=sobolMc.D1$stdev, n=sobolMc.D1$n))

g4 <- ggplot(combinedStdev.D1) + 
  geom_line(aes(x=n, y=stdev, colour=type)) +
  labs(title="D=1 Sobol & Uniform Stdev Side by Side by n", y="stdev") +
  myTheme
g4
```

```{r, echo=FALSE, fig.height=3}
combinedStdev.D2 <- data.frame(type="crude", stdev=crudeMc.D2$stdev, n=crudeMc.D2$n)
combinedStdev.D2 <- rbind(combinedStdev.D2, data.frame(type="sobol", stdev=sobolMc.D2$stdev, n=sobolMc.D2$n))

g4 <- ggplot(combinedStdev.D2) + 
  geom_line(aes(x=n, y=stdev, colour=type)) +
  labs(title="D=2 Sobol & Uniform Stdev Side by Side by n", y="stdev") +
  myTheme
g4
```

## c) Antithetic Variates

```{r antithetic-monte-carlo}
antitheticMC <- function(n, min, max, d = 1)
{
  theta.hat <- rep(NA, n)
  for(i in 1:n)
  {
    x <- runif(d, min, max)
    x2 <- 1 - x
    
    theta.hat[i] <- (costFx(x) + costFx(x2) ) / 2
  }
  
  return (theta.hat)
}

#ret <- antitheticMC(10, -5, 5, 2)
#ret
#mean(ret)
```

First we do the D=1 scenario:

```{r, cache=TRUE}
antitheticMc.D1 <- montecarlo.Loop(d=1, fun=antitheticMC)
```

```{r, echo=FALSE, fig.height=3}
g1 <- ggplot(antitheticMc.D1) + 
  geom_point(aes(x=n, y=mean), colour="lightblue") + 
  geom_line(aes(x=n, y=mean), colour="lightblue") + 
  geom_point(aes(x=n, y=stdev), colour="pink") + 
  geom_line(aes(x=n, y=stdev), colour="pink") + 
  geom_line(aes(x=n, y=EcActual), colour="lightgreen") + 
  labs(title="D=1 Antithetic Monte Carlo Estimate by n", y="mean/stdev") +
  myTheme
g1
```

```{r, echo=FALSE}
kable(antitheticMc.D1)
```

Next we do the D=2 scenario with the antithetic function:

```{r, cache=TRUE}
antitheticMc.D2 <- montecarlo.Loop(d=2, fun=antitheticMC)
```

```{r, echo=FALSE, fig.height=3}
g1 <- ggplot(antitheticMc.D2) + 
  geom_point(aes(x=n, y=mean), colour="lightblue") + 
  geom_line(aes(x=n, y=mean), colour="lightblue") + 
  geom_point(aes(x=n, y=stdev), colour="pink") + 
  geom_line(aes(x=n, y=stdev), colour="pink") + 
  geom_line(aes(x=n, y=EcActual), colour="lightgreen") + 
  labs(title="D=2 Antithetic Monte Carlo Estimate by n", y="mean/stdev") +
  myTheme
g1
```

```{r, echo=FALSE}
kable(antitheticMc.D2)
```

### Compare Pure Uniform and Antithetic

The antithetic scenario produces a smaller standard deviation (and therefore variance) in both the D=1 and D=2 cases. The visualizations below illustrate the point. The avarege values are generally the same, centering on the expected value.

```{r}
comparedMc2.D1 <- data.frame(crudeMc.D1$mean, 
                            antitheticMc.D1$mean, 
                            meanDiff=crudeMc.D1$mean - antitheticMc.D1$mean,
                            crudeMc.D1$stdev, 
                            antitheticMc.D1$stdev, 
                            stdevDiff=crudeMc.D1$stdev - antitheticMc.D1$stdev,
                            n=crudeMc.D1$n)
kable(comparedMc2.D1)
```

```{r, echo=FALSE, fig.height=3}
combinedStdev2.D1 <- data.frame(type="crude", stdev=crudeMc.D1$stdev, n=crudeMc.D1$n)
combinedStdev2.D1 <- rbind(combinedStdev2.D1, data.frame(type="antithetic", 
                                                       stdev=antitheticMc.D1$stdev, 
                                                       n=antitheticMc.D1$n))

g5 <- ggplot(combinedStdev2.D1) + 
  geom_line(aes(x=n, y=stdev, colour=type)) +
  labs(title="D=1 Antithetic & Uniform Stdev Side by Side by n", y="stdev") +
  myTheme
g5
```

```{r, echo=FALSE, fig.height=3}
combinedStdev2.D2 <- data.frame(type="crude", stdev=crudeMc.D2$stdev, n=crudeMc.D2$n)
combinedStdev2.D2 <- rbind(combinedStdev2.D2, data.frame(type="sobol", 
                                                        stdev=antitheticMc.D2$stdev, 
                                                        n=antitheticMc.D2$n))

g6 <- ggplot(combinedStdev2.D2) + 
  geom_line(aes(x=n, y=stdev, colour=type)) +
  labs(title="D=2 Antithetic & Uniform Stdev Side by Side by n", y="stdev") +
  myTheme
g6
```

## d) Latin Hypercube Sampling

```{r latinhypercube-monte-carlo}
latinHypercubeMC <- function(n, min, max, d = 1, K=10)
{
  #theta.hat <- rep(NA, n)
  classSize <- (max - min) / K
  samples <- n / K
  
  #print(paste("classSize:", classSize))
  #print(paste("samples:", samples))
  
  # Draw samples in the range of the stratum.
  Yi <- rep(NA, samples)
  for(i in 1:samples)
  {
    # Loop through each stratum
    Vk <- rep(NA, K)
    for(k in 1:K)
    {
      U <- runif(d, 
                 min + ((k - 1) * classSize), 
                 min + (k * classSize))
  
      #ndx <- ((k - 1) * samples) + s
      Vk[k] <- costFx(U)
    }
    
    Yi[i] <- mean(Vk)
  }
  
  
  return (Yi)
}

#ret <- latinHypercubeMC(100, -5, 5, 2)
#ret
#mean(ret)
```

First we do the D=1 scenario:

```{r, cache=TRUE}
latinHypercubeMc.D1 <- montecarlo.Loop(d=1, fun=latinHypercubeMC)
```

```{r, echo=FALSE, fig.height=3}
g1 <- ggplot(latinHypercubeMc.D1) + 
  geom_point(aes(x=n, y=mean), colour="lightblue") + 
  geom_line(aes(x=n, y=mean), colour="lightblue") + 
  geom_point(aes(x=n, y=stdev), colour="pink") + 
  geom_line(aes(x=n, y=stdev), colour="pink") + 
  geom_line(aes(x=n, y=EcActual), colour="lightgreen") + 
  labs(title="D=1 Latin Hypercube Monte Carlo Estimate by n", y="mean/stdev") +
  myTheme
g1
```

```{r, echo=FALSE}
kable(latinHypercubeMc.D1)
```

Then, the D=2 scenario. For some reason, this doesn't seem correct. The mean is not converging. Hmm...

```{r, cache=TRUE}
latinHypercubeMc.D2 <- montecarlo.Loop(d=2, fun=latinHypercubeMC)
```

```{r, echo=FALSE, fig.height=3}
g1 <- ggplot(latinHypercubeMc.D2) + 
  geom_point(aes(x=n, y=mean), colour="lightblue") + 
  geom_line(aes(x=n, y=mean), colour="lightblue") + 
  geom_point(aes(x=n, y=stdev), colour="pink") + 
  geom_line(aes(x=n, y=stdev), colour="pink") + 
  geom_line(aes(x=n, y=EcActual), colour="lightgreen") + 
  labs(title="D=2 Latin Hypercube Monte Carlo Estimate by n", y="mean/stdev") +
  myTheme
g1
```

```{r, echo=FALSE}
kable(latinHypercubeMc.D2)
```

## e) Importance Sampling

Next up, importance sampling. We define the function \(q(x)=c(x)p(x)/E_p[c(x)]\). This is usually not possible because we don't know the expected value \(E_p\) (it is what we are trying to calculate).

```{r}
qx <- function(x, d, min, max)
{
  cx <- costFx(x)
  Ecx <- (1/10)^d # since we know it already
  
  res <- (cx %*% t(px(x, min, max))) / Ecx
  
  return(res)
}

px <- function(x, min, max)
{
  return (dunif(x, min, max))
}

wx <- function(x, d, min, max)
{
  w <- px(x[1], min, max) / qx(x[1], d, min, max)
  return(w)
}
```

\[E_p[c(x)] = \int c(x) q(x) w(x) dx\]

\[q(x)=\frac{c(x)p(x)}{E_p[c(x)]}\]

\[w(x)=\frac{c(x)}{q(x)}\]

The \(q(x)\) denominator of \(w(x)\) would cancel out the \(q(x)\) in the case where we have the above functions leaving the \(c(x)\) of the \(w(x)\) remaining.

```{r}
# Helper function for the Accept/Reject approach
myRARmethod <- function(fun, min, max)
{
  M <- 2
  accepted <- FALSE
  while(!accepted)
  {
    # Get a random value from uniform distrubtion (g(x) for us)
    r <- runif(1, min, max)

    # Sample x from g(x) and u from U(0,1) (the uniform distribution over the unit interval)
    u <- runif(1, 0, 1)
    gx <- dunif(r, min, max)

    # Check whether or not u<f(x)/Mg(x). 
    if(u < fun(r, 1, min, max) / (M * gx))
    {
      accepted = TRUE
    }
  }

  return(r)
}

importanceSampleMC <- function(n, min, max, d = 1)
{
  theta.hat <- rep(NA, n)
  for(i in 1:n)
  {
    x <- rep(NA, d)
    for(j in 1:d)
    {
      x[j] <- myRARmethod(qx, min, max)
    }

    xw <- wx(x, d, min, max)
    theta.hat[i] <- costFx(x) * xw
  }
  
  return (theta.hat)
}
ret <- importanceSampleMC(20, -5, 5, 2)
ret
mean(ret)
```

First we do the D=1 scenario. Given the 0 standard deviation, this is certainly a minimum variance, but a bit surprised this is the case... maybe that is the expected result.

```{r, cache=TRUE}
importanceSampleMC.D1 <- montecarlo.Loop(d=1, fun=importanceSampleMC)
```

```{r, echo=FALSE, fig.height=3}
g1 <- ggplot(importanceSampleMC.D1) + 
  geom_point(aes(x=n, y=mean), colour="lightblue") + 
  geom_line(aes(x=n, y=mean), colour="lightblue") + 
  geom_point(aes(x=n, y=stdev), colour="pink") + 
  geom_line(aes(x=n, y=stdev), colour="pink") + 
  geom_line(aes(x=n, y=EcActual), colour="lightgreen") + 
  labs(title="D=1 Importance Sampling Monte Carlo Estimate by n", y="mean/stdev") +
  myTheme
g1
```

```{r, echo=FALSE}
kable(importanceSampleMC.D1)
```

Then, the D=2 scenario of importance sampling. Again, this is not converging so I guess I've missed something in my algorithm.

```{r, cache=TRUE}
importanceSampleMC.D2 <- montecarlo.Loop(d=2, fun=importanceSampleMC)
```

```{r, echo=FALSE, fig.height=3}
g1 <- ggplot(importanceSampleMC.D2) + 
  geom_point(aes(x=n, y=mean), colour="lightblue") + 
  geom_line(aes(x=n, y=mean), colour="lightblue") + 
  geom_point(aes(x=n, y=stdev), colour="pink") + 
  geom_line(aes(x=n, y=stdev), colour="pink") + 
  geom_line(aes(x=n, y=EcActual), colour="lightgreen") + 
  labs(title="D=2 Importance Sampling Monte Carlo Estimate by n", y="mean/stdev") +
  myTheme
g1
```

```{r, echo=FALSE}
kable(importanceSampleMC.D2)
```

## f) Summary

As previously shown, we saw variance reduction with each of the methods. The latin hypercube technique seemed to provide the most variance reduction at least in the D=1 case. 

# 2)

## 6.3

**Plot the power curves for the t-test in Example 6.9 for samples sizes 10, 20, 30, 40 and 50. but omit the standard error bars. Plot the curves on the same graph, each in a different color or different line type, and include a legend. Comment on the relation between power and sample size.**

```{r, cache=TRUE}
N<- c(10, 20, 30, 40, 50)
m <- 1000
mu0 <- 500
sigma <- 100
mu <- c(seq(450, 650, 10))
M <- length(mu)

dfPower <- data.frame(sample.size=c(), mu=c(), power=c())

for(n in N)
{
  for(i in 1:M)
  {
    mu1 <- mu[i]
    pvalues <- replicate(m, expr={
      # comment
      x <- rnorm(n, mean=mu1, sd=sigma)
      ttest <- t.test(x, alternative="greater", mu=mu0)
      ttest$p.value 
    })
    
    dfPower <- rbind(dfPower, data.frame(sample.size=n, mu=mu1, power=mean(pvalues <= 0.05)))
  }
}

```

As shown in the following visualization, the power values ramp up faster as the number of samples increases though there is a degree of diminishing returns on the ramp up.

```{r, fig.height=3, echo=FALSE}
dfPower$sample.size <- as.factor(dfPower$sample.size)
g21 <- ggplot(dfPower) + 
  geom_path(aes(x=mu, y=power, colour=sample.size)) + 
  labs(title="Power Curves") +
  myTheme
g21
```

### 6.4

**Suppose the \(X_1,...,X_n\) are a random sample from a lognormal distribution with unknown parameters. Construct a 95% confidence interval for the parameter \(\mu\). Use a Monte Carlo method to obtain an empirical estimate of the confidence level.**

The following code runs a Monte Carlo simulation of using random log normal variables to estimate the \(\mu\) parameter and produce a confidence level.

```{r}
m < 1000
n <- 2500
mu <- 1
sigma <- 0.5
alpha <- 0.05

res <- replicate(m, expr={
  x <- rlnorm(n, meanlog=mu, sdlog=sigma)
  
  log.mean <- mean(log(x))
  log.sd <- sd(log(x))
  log.se <- log.sd / sqrt(n)
  t.val <- qt(1 - alpha / 2, n - 1)
  
  CI <- c(log.mean, log.mean - (t.val * log.se), log.mean + (t.val * log.se))
  CI
  
  #log.mean
  #log.sd
})

theCI <- c(mean(res[2,]), mean(res[3,]))
theCI

theCL <- mean(res[2,] < (0.999) & (1.001) < res[3,])
theCL
```

### 7.1

**Compute the jackknife esimate of the bias and the standard error of the correlation statistic in Example 7.2**

The following code uses jackknife to compute the bias of the law data:

```{r}
library(bootstrap)

n <- nrow(law)
theta.hat <- mean(law$LSAT) / mean(law$GPA)
theta.hat

theta.jack <- numeric(n)

for(i in 1:n)
{
  theta.jack[i] <- mean(law$LSAT[-i] / mean(law$GPA[-i]))
}
bias <- (n-1) * (mean(theta.jack) - theta.hat)
bias
```

Next we estimate the standard error of the correlation statistic:

```{r}
#cor.theta.hat <- cor(law$LSAT, law$GPA)
#cor.theta.hat

cor.theta.jack <- numeric(n)

for(i in 1:n)
{
  cor.theta.jack[i] <- cor(law$LSAT[-i] , law$GPA[-i])
}
se.cor.jack <- sqrt( ((n-1)) * mean( ( cor.theta.jack - mean(cor.theta.jack))^2 ) )
se.cor.jack
```

### 7.4

**Refer to the air-conditioning data set aircondit provided in the boot package. The 12 observations are the times in hours between failures of air-conditioning equipment. Assume that the times between failures follow an exponential model \(Exp(\lambda)\). Obtain the MLE of the hazard rate \(\lambda\) and use bootstrap to estimate the bias and standard error of the estimate.**

```{r}
library(boot)
head(aircondit)

B <- 10
n <- length(aircondit)

lambda.hat <- n / sum(aircondit)
lambda.hat

theta.boot <- numeric(B)
for(b in 1:B)
{
  x <- sample(aircondit, size=n, replace = TRUE)
  theta.boot[b] <- n / sum(x) 
}
# Standard error of estimate of lambda
se.lambda.boot <- sd(theta.boot)
se.lambda.boot


```
